{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGFG86G217fM",
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Overview</a></span></li><li><span><a href=\"#Models\" data-toc-modified-id=\"Models-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Linear-regression\" data-toc-modified-id=\"Linear-regression-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Linear regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-normal-equation\" data-toc-modified-id=\"The-normal-equation-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>The normal equation</a></span></li><li><span><a href=\"#Batch-gradient-descent-(BGD)\" data-toc-modified-id=\"Batch-gradient-descent-(BGD)-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Batch gradient descent (BGD)</a></span></li><li><span><a href=\"#Stochastic-gradient-descent-(SGD)\" data-toc-modified-id=\"Stochastic-gradient-descent-(SGD)-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>Stochastic gradient descent (SGD)</a></span></li><li><span><a href=\"#Mini-batch-gradient-descent-(MBGD)\" data-toc-modified-id=\"Mini-batch-gradient-descent-(MBGD)-2.1.4\"><span class=\"toc-item-num\">2.1.4&nbsp;&nbsp;</span>Mini-batch gradient descent (MBGD)</a></span></li></ul></li><li><span><a href=\"#Logistic-regression\" data-toc-modified-id=\"Logistic-regression-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Logistic regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Mini-batch-gradient-descent-(MBGD)\" data-toc-modified-id=\"Mini-batch-gradient-descent-(MBGD)-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Mini-batch gradient descent (MBGD)</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0iSrdHP17fP"
   },
   "source": [
    "<b>\n",
    "<p>\n",
    "<center>\n",
    "<font size=\"5\">\n",
    "Popular Machine Learning Methods: Idea, Practice and Math\n",
    "</font>\n",
    "</center>\n",
    "</p>\n",
    "    \n",
    "<p>\n",
    "<center>\n",
    "<font size=\"4\">\n",
    "Models: Shallow Learning\n",
    "</font>\n",
    "</center>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "<font size=\"3\">\n",
    "Data Science, Columbian College of Arts & Sciences, George Washington University\n",
    "</font>\n",
    "</center>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "<font size=\"3\">\n",
    "Yuxiao Huang\n",
    "</font>\n",
    "</center>\n",
    "</p>\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "froUMaTq17fQ"
   },
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rs8b9gZt17fS"
   },
   "source": [
    "- This notebook includes some common models used in PMLM.\n",
    "- Concretely, these models are:\n",
    "    - linear regression\n",
    "    - logistic regression\n",
    "    - single / multiple layer perceptron \n",
    "- See the accompanied slides in our [github repository](https://github.com/yuxiaohuang/teaching/tree/master/gwu/machine_learning_I/fall_2020/slides)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AuNO5H9y17fU"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXOL0bd017fW"
   },
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GfS9Suw17fX"
   },
   "source": [
    "### The normal equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQaNdBpD17fZ"
   },
   "source": [
    "The code below shows how to implement linear regression using the normal equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 650,
     "status": "ok",
     "timestamp": 1601408797799,
     "user": {
      "displayName": "Huang Yuxiao",
      "photoUrl": "",
      "userId": "05167076769245149404"
     },
     "user_tz": 240
    },
    "id": "rRQpBrAI17fd"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import numpy as np\n",
    "\n",
    "class LinearRegression_NE(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"Linear regression implemented using the normal equation\"\"\"\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        The fit function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : the feature matrix\n",
    "        y : the target vector\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the augmented feature matrix, [1, X]\n",
    "        IX = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "        # Get the optimal solution using the normal equation\n",
    "        self.theta = np.linalg.pinv(IX).dot(y)\n",
    "        \n",
    "        # Get the predicted target vector\n",
    "        y_pred = self.net_input(IX)\n",
    "                        \n",
    "        # Get the loss (MSE)\n",
    "        self.loss = ((y - y_pred) ** 2).sum() / IX.shape[0]\n",
    "        \n",
    "    def net_input(self, IX):\n",
    "        \"\"\"\n",
    "        Get the predicted target vector\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        IX : The augmented feature matrix [1, X]\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        The predicted target vector\n",
    "        \"\"\"\n",
    "        \n",
    "        return IX.dot(self.theta)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        The predict function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : the feature matrix\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        The predicted value of the target\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the augmented feature matrix [1, X]\n",
    "        IX = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        \n",
    "        return self.net_input(IX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHZAarvd17fk"
   },
   "source": [
    "### Batch gradient descent (BGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mH7feza17fm"
   },
   "source": [
    "The code below shows how to implement linear regression using batch gradient descent and regularization (lasso, ridge and elastic net)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 645,
     "status": "ok",
     "timestamp": 1601408797800,
     "user": {
      "displayName": "Huang Yuxiao",
      "photoUrl": "",
      "userId": "05167076769245149404"
     },
     "user_tz": 240
    },
    "id": "qYsTNToH17fn"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import numpy as np\n",
    "\n",
    "class LinearRegression_BGD(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"Linear regression implemented using batch gradient descent and regularization (lasso, ridge and elastic net)\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 max_iter=100, \n",
    "                 eta=10 ** -2,\n",
    "                 penalty='l2',\n",
    "                 alpha=0.0001, \n",
    "                 gamma=0.15,\n",
    "                 random_state=42):\n",
    "        \n",
    "        # The maximum number of epochs\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "        # The learning rate\n",
    "        self.eta = eta\n",
    "        \n",
    "        # The regularization term\n",
    "        self.penalty=penalty\n",
    "        \n",
    "        # The regularization parameter\n",
    "        self.alpha=alpha\n",
    "\n",
    "        # The elastic net mixing parameter\n",
    "        self.gamma=gamma\n",
    "\n",
    "        # The random state\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        The fit function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : The training feature matrix\n",
    "        y_train : The training target vector\n",
    "        X_val : The validation feature matrix\n",
    "        y_val : The validation target vector\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the augmented training feature matrix, [1, X_train]\n",
    "        IX_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "        \n",
    "        # Get the random number generator\n",
    "        self.rgen = np.random.RandomState(seed=self.random_state)\n",
    "        \n",
    "        # Initialize the parameters\n",
    "        self.theta = self.rgen.normal(loc=0.0, scale=0.01, size=IX_train.shape[1])\n",
    "        \n",
    "        # Initialize the training and validation loss\n",
    "        self.loss_train, self.loss_val = [], []\n",
    "        \n",
    "        # For each epoch\n",
    "        for _ in range(self.max_iter):\n",
    "            # Get the predicted target vector on the training data\n",
    "            y_train_pred = self.net_input(IX_train)\n",
    "            \n",
    "            # Get the training error\n",
    "            error_train = y_train - y_train_pred\n",
    "                        \n",
    "            # Get the training mse\n",
    "            mse_train = (error_train ** 2).sum() / IX_train.shape[0]\n",
    "            \n",
    "            # Update the parameters\n",
    "            # If no regularization\n",
    "            if self.penalty == None:\n",
    "                self.theta += self.eta * (2 / IX_train.shape[0] * IX_train.T.dot(error_train))\n",
    "            # If lasso\n",
    "            elif self.penalty == 'l1':\n",
    "                self.theta += self.eta * (2 / IX_train.shape[0] * IX_train.T.dot(error_train) \n",
    "                                          - self.alpha * np.append([0], np.sign(self.theta[1:])))\n",
    "            # If ridge\n",
    "            elif self.penalty == 'l2':\n",
    "                self.theta += self.eta * (2 / IX_train.shape[0] * IX_train.T.dot(error_train) \n",
    "                                          - self.alpha * np.append([0], self.theta[1:]))\n",
    "            # If elastic net\n",
    "            elif self.penalty == 'elasticnet':\n",
    "                self.theta += self.eta * (2 / IX_train.shape[0] * IX_train.T.dot(error_train) \n",
    "                                          - self.alpha * self.gamma * np.append([0], np.sign(self.theta[1:])) \n",
    "                                          - self.alpha * (1 - self.gamma) * np.append([0], self.theta[1:]))\n",
    "                           \n",
    "            # Update the training loss\n",
    "            self.loss_train.append(mse_train)\n",
    "            \n",
    "            # If the validation feature matrix and target vector are available\n",
    "            if X_val is not None and y_val is not None:\n",
    "                # Get the augmented validation feature matrix, [1, X_val]\n",
    "                IX_val = np.hstack((np.ones((X_val.shape[0], 1)), X_val))\n",
    "                \n",
    "                # Get the predicted target vector on the validation data\n",
    "                y_val_pred = self.net_input(IX_val)\n",
    "                \n",
    "                # Get the validation error\n",
    "                error_val = y_val - y_val_pred\n",
    "                \n",
    "                # Get the validation mse\n",
    "                mse_val = (error_val ** 2).sum() / IX_val.shape[0]\n",
    "                \n",
    "                # Update the validation loss\n",
    "                self.loss_val.append(mse_val)\n",
    "\n",
    "    def net_input(self, IX):\n",
    "        \"\"\"\n",
    "        Get the predicted target vector\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        IX : The augmented feature matrix [1, X]\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        The predicted target vector\n",
    "        \"\"\"\n",
    "        \n",
    "        return IX.dot(self.theta)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        The predict function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : the feature matrix\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        The predicted value of the target\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the augmented feature matrix [1, X]\n",
    "        IX = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        \n",
    "        return self.net_input(IX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zufbpy-S17fs"
   },
   "source": [
    "### Stochastic gradient descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3iwyaosu17fs"
   },
   "source": [
    "The code below shows how to implement linear regression using stochastic gradient descent and regularization (lasso, ridge and elastic net)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 795,
     "status": "ok",
     "timestamp": 1601408797952,
     "user": {
      "displayName": "Huang Yuxiao",
      "photoUrl": "",
      "userId": "05167076769245149404"
     },
     "user_tz": 240
    },
    "id": "CPNHUh7S17fu"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import numpy as np\n",
    "\n",
    "class LinearRegression_SGD(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"Linear regression implemented using stochastic gradient descent and regularization (lasso, ridge and elastic net)\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 max_iter=100,\n",
    "                 shuffle=True,\n",
    "                 eta=10 ** -2, \n",
    "                 penalty='l2',\n",
    "                 alpha=0.0001, \n",
    "                 gamma=0.15,\n",
    "                 random_state=42):\n",
    "        \n",
    "        # The maximum number of epochs\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "        # Whether to shuffle samples in each epoch\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        # The learning rate\n",
    "        self.eta = eta\n",
    "        \n",
    "        # The regularization term\n",
    "        self.penalty=penalty\n",
    "        \n",
    "        # The regularization parameter\n",
    "        self.alpha=alpha\n",
    "\n",
    "        # The elastic net mixing parameter\n",
    "        self.gamma=gamma\n",
    "        \n",
    "        # The random state\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        The fit function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : The training feature matrix\n",
    "        y_train : The training target vector\n",
    "        X_val : The validation feature matrix\n",
    "        y_val : The validation target vector\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the augmented training feature matrix, [1, X_train]\n",
    "        IX_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "        \n",
    "        # Get the indices of the augmented training feature matrix\n",
    "        idxs_train = np.array(range(IX_train.shape[0]))\n",
    "        \n",
    "        # Get the random number generator\n",
    "        self.rgen = np.random.RandomState(seed=self.random_state)\n",
    "        \n",
    "        # Initialize the parameters\n",
    "        self.theta = self.rgen.normal(loc=0.0, scale=0.01, size=IX_train.shape[1])\n",
    "        \n",
    "        # Initialize the training and validation loss\n",
    "        self.loss_train, self.loss_val = [], []\n",
    "        \n",
    "        # For each epoch\n",
    "        for _ in range(self.max_iter):\n",
    "            if self.shuffle is True:\n",
    "                # Shuffle the indices\n",
    "                self.rgen.shuffle(idxs_train)\n",
    "                \n",
    "            # Initialize the mse\n",
    "            mse_train = 0\n",
    "            \n",
    "            # For each sample\n",
    "            for i in idxs_train:                \n",
    "                # Get the predicted target vector on the training data\n",
    "                y_train_pred = self.net_input(IX_train[i, :])\n",
    "\n",
    "                # Get the training error\n",
    "                error_train = y_train[i] - y_train_pred\n",
    "\n",
    "                # Get the training mse\n",
    "                mse_train += (error_train ** 2) / IX_train.shape[0]\n",
    "\n",
    "                # Update the parameters\n",
    "                # If no regularization\n",
    "                if self.penalty == None:\n",
    "                    self.theta += self.eta * (2 * IX_train[i, :].T.dot(error_train))\n",
    "                # If lasso\n",
    "                elif self.penalty == 'l1':\n",
    "                    self.theta += self.eta * (2 * IX_train[i, :].T.dot(error_train) \n",
    "                                              - self.alpha * np.append([0], np.sign(self.theta[1:])))\n",
    "                # If ridge\n",
    "                elif self.penalty == 'l2':\n",
    "                    self.theta += self.eta * (2 * IX_train[i, :].T.dot(error_train) \n",
    "                                              - self.alpha * np.append([0], self.theta[1:]))\n",
    "                # If elastic net\n",
    "                elif self.penalty == 'elasticnet':\n",
    "                    self.theta += self.eta * (2 * IX_train[i, :].T.dot(error_train) \n",
    "                                              - self.alpha * self.gamma * np.append([0], np.sign(self.theta[1:])) \n",
    "                                              - self.alpha * (1 - self.gamma) * np.append([0], self.theta[1:]))\n",
    "\n",
    "            # Update the training loss\n",
    "            self.loss_train.append(mse_train)\n",
    "\n",
    "            # If the validation feature matrix and target vector are available\n",
    "            if X_val is not None and y_val is not None:\n",
    "                # Get the augmented validation feature matrix, [1, X_val]\n",
    "                IX_val = np.hstack((np.ones((X_val.shape[0], 1)), X_val))\n",
    "\n",
    "                # Get the predicted target vector on the validation data\n",
    "                y_val_pred = self.net_input(IX_val)\n",
    "\n",
    "                # Get the validation error\n",
    "                error_val = y_val - y_val_pred\n",
    "\n",
    "                # Get the validation mse\n",
    "                mse_val = (error_val ** 2).sum() / IX_val.shape[0]\n",
    "\n",
    "                # Update the validation loss\n",
    "                self.loss_val.append(mse_val)\n",
    "                \n",
    "    def net_input(self, IX):\n",
    "        \"\"\"\n",
    "        Get the predicted target vector\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        IX : The augmented feature matrix [1, X]\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        The predicted target vector\n",
    "        \"\"\"\n",
    "        \n",
    "        return IX.dot(self.theta)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        The predict function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : the feature matrix\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        The predicted value of the target\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the augmented feature matrix [1, X]\n",
    "        IX = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        \n",
    "        return self.net_input(IX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alEChMIu17fy"
   },
   "source": [
    "### Mini-batch gradient descent (MBGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1U0dTDvc17fy"
   },
   "source": [
    "The code below shows how to implement linear regression using mini-batch gradient descent and regularization (lasso, ridge and elastic net)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 989,
     "status": "ok",
     "timestamp": 1601408798149,
     "user": {
      "displayName": "Huang Yuxiao",
      "photoUrl": "",
      "userId": "05167076769245149404"
     },
     "user_tz": 240
    },
    "id": "2ZljWGWn17fz",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import numpy as np\n",
    "\n",
    "class LinearRegression_MBGD(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"Linear regression implemented using mini-batch gradient descent and regularization (lasso, ridge and elastic net)\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 max_iter=100,\n",
    "                 shuffle=True,\n",
    "                 batch_size=32,\n",
    "                 eta=10 ** -2, \n",
    "                 penalty='l2',\n",
    "                 alpha=0.0001, \n",
    "                 gamma=0.15,\n",
    "                 random_state=42):\n",
    "        \n",
    "        # The maximum number of epochs\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "        # Whether to shuffle samples in each epoch\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        # The size of minibatches for stochastic optimizers\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # The learning rate\n",
    "        self.eta = eta\n",
    "        \n",
    "        # The regularization term\n",
    "        self.penalty=penalty\n",
    "        \n",
    "        # The regularization parameter\n",
    "        self.alpha=alpha\n",
    "\n",
    "        # The elastic net mixing parameter\n",
    "        self.gamma=gamma\n",
    "        \n",
    "        # The random state\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        The fit function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : The training feature matrix\n",
    "        y_train : The training target vector\n",
    "        X_val : The validation feature matrix\n",
    "        y_val : The validation target vector\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the augmented training feature matrix, [1, X_train]\n",
    "        IX_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "        \n",
    "        # Get the random number generator\n",
    "        self.rgen = np.random.RandomState(seed=self.random_state)\n",
    "        \n",
    "        # Initialize the parameters\n",
    "        self.theta = self.rgen.normal(loc=0.0, scale=0.01, size=IX_train.shape[1])\n",
    "        \n",
    "        # Initialize the training and validation loss\n",
    "        self.loss_train, self.loss_val = [], []\n",
    "        \n",
    "        # For each epoch\n",
    "        for _ in range(self.max_iter):\n",
    "            # Get the indices of the training data\n",
    "            idxs_train = np.array(range(IX_train.shape[0]))\n",
    "            \n",
    "            # Get the minibatches of the training data\n",
    "            mbs = self.get_minibatches(idxs_train)\n",
    "\n",
    "            # Initialize the training mse\n",
    "            mse_train = 0\n",
    "\n",
    "            # For each minibatch\n",
    "            for mb in mbs:   \n",
    "                # Get the augmented training feature matrix and target vector\n",
    "                IX_train_mb, y_train_mb = IX_train[mb,:], y_train[mb]\n",
    "\n",
    "                # Get the predicted target vector on the training data\n",
    "                y_train_mb_pred = self.net_input(IX_train_mb)\n",
    "\n",
    "                # Get the training error\n",
    "                error_train = y_train_mb - y_train_mb_pred\n",
    "\n",
    "                # Get the training mse\n",
    "                mse_train += (error_train ** 2).sum() / IX_train.shape[0]\n",
    "                \n",
    "                # Update the parameters\n",
    "                # If no regularization\n",
    "                if self.penalty == None:\n",
    "                    self.theta += self.eta * (2 / IX_train_mb.shape[0] * IX_train_mb.T.dot(error_train))\n",
    "                # If lasso\n",
    "                elif self.penalty == 'l1':\n",
    "                    self.theta += self.eta * (2 / IX_train_mb.shape[0] * IX_train_mb.T.dot(error_train) \n",
    "                                              - self.alpha * np.append([0], np.sign(self.theta[1:])))\n",
    "                # If ridge\n",
    "                elif self.penalty == 'l2':\n",
    "                    self.theta += self.eta * (2 / IX_train_mb.shape[0] * IX_train_mb.T.dot(error_train) \n",
    "                                              - self.alpha * np.append([0], self.theta[1:]))\n",
    "                # If elastic net\n",
    "                elif self.penalty == 'elasticnet':\n",
    "                    self.theta += self.eta * (2 / IX_train_mb.shape[0] * IX_train_mb.T.dot(error_train) \n",
    "                                              - self.alpha * self.gamma * np.append([0], np.sign(self.theta[1:])) \n",
    "                                              - self.alpha * (1 - self.gamma) * np.append([0], self.theta[1:]))\n",
    "\n",
    "            # Update the training loss\n",
    "            self.loss_train.append(mse_train)\n",
    "            \n",
    "            # If the validation feature matrix and target vector are available\n",
    "            if X_val is not None and y_val is not None:\n",
    "                # Get the augmented validation feature matrix, [1, X_val]\n",
    "                IX_val = np.hstack((np.ones((X_val.shape[0], 1)), X_val))\n",
    "                \n",
    "                # Get the predicted target vector on the validation data\n",
    "                y_val_pred = self.net_input(IX_val)\n",
    "                \n",
    "                # Get the validation error\n",
    "                error_val = y_val - y_val_pred\n",
    "                \n",
    "                # Get the validation mse\n",
    "                mse_val = (error_val ** 2).sum() / IX_val.shape[0]\n",
    "                \n",
    "                # Update the validation loss\n",
    "                self.loss_val.append(mse_val)\n",
    "\n",
    "    def get_minibatches(self, idxs):\n",
    "        \"\"\"\n",
    "        Get the minibatches\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        idxs : The indices of the data\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        The minibatches\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize the minibatches\n",
    "        mbs = []\n",
    "        \n",
    "        if self.shuffle is True:\n",
    "            # Shuffle the indices\n",
    "            self.rgen.shuffle(idxs)\n",
    "                \n",
    "        # Get the number of minibatches\n",
    "        n_batch = len(idxs) // self.batch_size\n",
    "        \n",
    "        # For each minibatch\n",
    "        for i in range(n_batch):\n",
    "            # Get the first and last index (exclusive) of the minibatch\n",
    "            first_idx = i * self.batch_size\n",
    "            last_idx = min((i + 1) * self.batch_size, len(idxs))\n",
    "                                    \n",
    "            # Get the minibatch\n",
    "            mb = idxs[first_idx : last_idx]\n",
    "            \n",
    "            # Update the minibatches\n",
    "            mbs.append(mb)\n",
    "\n",
    "        return mbs\n",
    "\n",
    "    def net_input(self, IX):\n",
    "        \"\"\"\n",
    "        Get the predicted target vector\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        IX : The augmented feature matrix [1, X]\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        The predicted target vector\n",
    "        \"\"\"\n",
    "        \n",
    "        return IX.dot(self.theta)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        The predict function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : the feature matrix\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        The predicted value of the target\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the augmented feature matrix [1, X]\n",
    "        IX = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        \n",
    "        return self.net_input(IX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVP7unFGBRlz"
   },
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqph1EErBRl3"
   },
   "source": [
    "### Mini-batch gradient descent (MBGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLrJ4jJJBRl3"
   },
   "source": [
    "The code below shows how to implement logistic regression using mini-batch gradient descent and regularization (lasso, ridge and elastic net)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 1376,
     "status": "ok",
     "timestamp": 1601408798539,
     "user": {
      "displayName": "Huang Yuxiao",
      "photoUrl": "",
      "userId": "05167076769245149404"
     },
     "user_tz": 240
    },
    "id": "OIA_IwdiBRl4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import numpy as np\n",
    "\n",
    "class LogisticRegression_MBGD(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"Logistic regression implemented using mini-batch gradient descent and regularization (lasso, ridge and elastic net)\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 max_iter=100,\n",
    "                 shuffle=True,\n",
    "                 batch_size=32,\n",
    "                 eta=10 ** -2, \n",
    "                 penalty='l2',\n",
    "                 alpha=1, \n",
    "                 gamma=0.5,\n",
    "                 random_state=42):\n",
    "        \n",
    "        # The maximum number of epochs\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "        # Whether to shuffle samples in each epoch\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        # The size of minibatches for stochastic optimizers\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # The learning rate\n",
    "        self.eta = eta\n",
    "        \n",
    "        # The regularization term\n",
    "        self.penalty=penalty\n",
    "        \n",
    "        # The regularization parameter\n",
    "        self.alpha=alpha\n",
    "\n",
    "        # The elastic net mixing parameter\n",
    "        self.gamma=gamma\n",
    "        \n",
    "        # The random state\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        The fit function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : The training feature matrix\n",
    "        y_train : The training target vector\n",
    "        X_val : The validation feature matrix\n",
    "        y_val : The validation target vector\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the augmented training feature matrix, [1, X_train]\n",
    "        IX_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "        \n",
    "        # Get the random number generator\n",
    "        self.rgen = np.random.RandomState(seed=self.random_state)\n",
    "        \n",
    "        # Get the unique classes of the target\n",
    "        self.classes = np.unique(y_train)\n",
    "        \n",
    "        # Get the number of unique classes of the target\n",
    "        self.n_classes = len(self.classes)\n",
    "        \n",
    "        # If binary classification\n",
    "        if self.n_classes == 2:\n",
    "            # Make a copy of y_train\n",
    "            Y_train = np.copy(y_train)       \n",
    "        # If multi-class classification\n",
    "        else:\n",
    "            # Get the one-hot-encoded training target matrix\n",
    "            Y_train = pd.get_dummies(y_train).values\n",
    "        \n",
    "        # Initialize the parameters\n",
    "        # If binary classification\n",
    "        if self.n_classes == 2:\n",
    "            self.theta = self.rgen.normal(loc=0.0, scale=0.01, size=IX_train.shape[1])\n",
    "        # If multi-class classification\n",
    "        else:\n",
    "            self.theta = self.rgen.normal(loc=0.0, scale=0.01, size=(IX_train.shape[1], self.n_classes))            \n",
    "        \n",
    "        # Initialize the training and validation loss\n",
    "        self.loss_train, self.loss_val = [], []\n",
    "        \n",
    "        # For each epoch\n",
    "        for _ in range(self.max_iter):\n",
    "            # Get the indices of the training data\n",
    "            idxs_train = np.array(range(IX_train.shape[0]))\n",
    "            \n",
    "            # Get the minibatches of the training data\n",
    "            mbs = self.get_minibatches(idxs_train)\n",
    "\n",
    "            # Initialize the training mse\n",
    "            mse_train = 0\n",
    "\n",
    "            # For each minibatch\n",
    "            for mb in mbs:   \n",
    "                # Get the augmented training feature matrix and target matrix\n",
    "                IX_train_mb, Y_train_mb = IX_train[mb,:], Y_train[mb]\n",
    "                                          \n",
    "                # Get the net input matrix\n",
    "                N_train_mb = self.net_input(IX_train_mb)\n",
    "                \n",
    "                # Get the probability matrix\n",
    "                P_train_mb = self.activation(N_train_mb)\n",
    "                                          \n",
    "                # Get the training error\n",
    "                error_train = Y_train_mb - P_train_mb\n",
    "\n",
    "                # Get the training mse\n",
    "                mse_train += (error_train ** 2).sum() / IX_train.shape[0]\n",
    "                \n",
    "                # Update the parameters\n",
    "                # If no regularization\n",
    "                if self.penalty == None:\n",
    "                    self.theta += self.eta / IX_train_mb.shape[0] * (IX_train_mb.T.dot(error_train))\n",
    "                # If lasso\n",
    "                elif self.penalty == 'l1':\n",
    "                    # If binary classification\n",
    "                    if self.n_classes == 2:\n",
    "                        self.theta += self.eta / IX_train_mb.shape[0] * (IX_train_mb.T.dot(error_train) \n",
    "                                                                        - self.alpha * np.append([0], np.sign(self.theta[1:])))\n",
    "                    # If multi-class classification\n",
    "                    else:\n",
    "                        self.theta += self.eta / IX_train_mb.shape[0] * (IX_train_mb.T.dot(error_train) \n",
    "                                                                        - self.alpha * np.append(np.zeros((1, self.theta.shape[1])), np.sign(self.theta[1:,:]), axis=0))                        \n",
    "                # If ridge\n",
    "                elif self.penalty == 'l2':\n",
    "                    # If binary classification\n",
    "                    if self.n_classes == 2:\n",
    "                        self.theta += self.eta / IX_train_mb.shape[0] * (IX_train_mb.T.dot(error_train) \n",
    "                                                                        - self.alpha * np.append([0], self.theta[1:]))\n",
    "                    # If multi-class classification\n",
    "                    else:\n",
    "                        self.theta += self.eta / IX_train_mb.shape[0] * (IX_train_mb.T.dot(error_train) \n",
    "                                                                        - self.alpha * np.append(np.zeros((1, self.theta.shape[1])), self.theta[1:,:], axis=0))                        \n",
    "                # If elastic net\n",
    "                elif self.penalty == 'elasticnet':\n",
    "                    # If binary classification\n",
    "                    if self.n_classes == 2:\n",
    "                        self.theta += self.eta / IX_train_mb.shape[0] * (IX_train_mb.T.dot(error_train) \n",
    "                                                                        - self.alpha * self.gamma * np.append([0], np.sign(self.theta[1:])) \n",
    "                                                                        - self.alpha * (1 - self.gamma) * np.append([0], self.theta[1:]))\n",
    "                    # If multi-class classification\n",
    "                    else:\n",
    "                        self.theta += self.eta / IX_train_mb.shape[0] * (IX_train_mb.T.dot(error_train) \n",
    "                                                                        - self.alpha * self.gamma * np.append(np.zeros((1, self.theta.shape[1])), np.sign(self.theta[1:,:]), axis=0) \n",
    "                                                                        - self.alpha * (1 - self.gamma) * np.append(np.zeros((1, self.theta.shape[1])), self.theta[1:,:], axis=0))\n",
    "                                                \n",
    "            # Update the training loss\n",
    "            self.loss_train.append(mse_train)\n",
    "            \n",
    "            # If the validation feature matrix and target vector are available\n",
    "            if X_val is not None and y_val is not None:\n",
    "                # Get the augmented validation feature matrix, [1, X_val]\n",
    "                IX_val = np.hstack((np.ones((X_val.shape[0], 1)), X_val))\n",
    "\n",
    "                # If binary classification\n",
    "                if self.n_classes == 2:\n",
    "                    # Make a copy of y_val\n",
    "                    Y_val = np.copy(y_val)            \n",
    "                # If multi-class classification\n",
    "                else:\n",
    "                    # Get the one-hot-encoded validation target matrix\n",
    "                    Y_val = pd.get_dummies(y_val).values\n",
    "                \n",
    "                # Get the net input matrix on the validation data\n",
    "                N_val = self.net_input(IX_val)\n",
    "                \n",
    "                # Get the probability matrix on the validation data\n",
    "                P_val = self.activation(N_val)\n",
    "                                          \n",
    "                # Get the validation error\n",
    "                error_val = Y_val - P_val\n",
    "                \n",
    "                # Get the validation mse\n",
    "                mse_val = (error_val ** 2).sum() / IX_val.shape[0]\n",
    "                \n",
    "                # Update the validation loss\n",
    "                self.loss_val.append(mse_val)\n",
    "\n",
    "    def get_minibatches(self, idxs):\n",
    "        \"\"\"\n",
    "        Get the minibatches\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        idxs : The indices of the data\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        The minibatches\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize the minibatches\n",
    "        mbs = []\n",
    "        \n",
    "        if self.shuffle is True:\n",
    "            # Shuffle the indices\n",
    "            self.rgen.shuffle(idxs)\n",
    "                \n",
    "        # Get the number of minibatches\n",
    "        n_batch = len(idxs) // self.batch_size\n",
    "        \n",
    "        # For each minibatch\n",
    "        for i in range(n_batch):\n",
    "            # Get the first and last index (exclusive) of the minibatch\n",
    "            first_idx = i * self.batch_size\n",
    "            last_idx = min((i + 1) * self.batch_size, len(idxs))\n",
    "                                    \n",
    "            # Get the minibatch\n",
    "            mb = idxs[first_idx : last_idx]\n",
    "            \n",
    "            # Update the minibatches\n",
    "            mbs.append(mb)\n",
    "\n",
    "        return mbs\n",
    "\n",
    "    def net_input(self, IX):\n",
    "        \"\"\"\n",
    "        Get the net input matrix\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        IX : The augmented feature matrix [1, X]\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        The net input matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        return IX.dot(self.theta)\n",
    "    \n",
    "    def activation(self, net_input):\n",
    "        \"\"\"\n",
    "        Get the probability (sigmoid or softmax) matrix\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        net_input : The net input\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        The probability (sigmoid or softmax) matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        # If binary classification\n",
    "        if self.n_classes == 2:\n",
    "            # Get the exponent of the negative net input\n",
    "            neg_net_input_exp = np.exp(-np.clip(net_input, -250, 250))\n",
    "            \n",
    "            # Return the sigmoid matrix\n",
    "            return 1. / (1. + neg_net_input_exp)           \n",
    "        # If multi-class classification\n",
    "        else:\n",
    "            # Get the exponent of the net input\n",
    "            net_input_exp = np.exp(net_input - np.max(net_input, axis=1).reshape(-1, 1))\n",
    "\n",
    "            # Return the softmax matrix\n",
    "            return net_input_exp / np.sum(net_input_exp, axis=1).reshape(-1, 1)\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        The predict probability function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : the feature matrix\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        The probability (sigmoid or softmax) matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the augmented feature matrix [1, X]\n",
    "        IX = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "            \n",
    "        # Get the net_input matrix\n",
    "        N = self.net_input(IX)\n",
    "\n",
    "        return self.activation(N)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        The predict class function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : the feature matrix\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        The predicted class vector\n",
    "        \"\"\"\n",
    "        \n",
    "        # If binary classification\n",
    "        if self.n_classes == 2:\n",
    "            return (self.predict_proba(X) >= 0.5) * 1         \n",
    "        # If multi-class classification\n",
    "        else:\n",
    "            return np.argmax(self.predict_proba(X), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 1374,
     "status": "ok",
     "timestamp": 1601408798539,
     "user": {
      "displayName": "Huang Yuxiao",
      "photoUrl": "",
      "userId": "05167076769245149404"
     },
     "user_tz": 240
    },
    "id": "gnRPDPDGBRl5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "pmlm_models_shallow.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
